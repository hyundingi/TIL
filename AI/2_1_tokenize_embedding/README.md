# 딥러닝 및 이미지 모델 - 이미지 모델

- 이미지 기반 학습 모델의 구조와 작동 방식을 이해합니다.
- 대표적 CNN 기반 이미지 모델의 변천사를 배웁니다.
- 이미지 분류 문제에서 실습을 통해 주요 이미지 모델을 적용해봅니다.

## 1. CNN 살펴보기

현재 기업의 불량품 찾는데 많이 사용되고 있다.

범죄자 찾기, 등 

### 1-1. CNN vs FCN

차원을 반드시 주의해야한다.

filter는 항상 입력의 깊이 / 채널 축과 동일한 차원이어야 한다.

Convolution : 필터를 이미지 상에서 이동시키면서 내적 반복 수행, 내적으로 구한 모든 값을 출력으로 제공

출력 해상도 = 입력 해상도 - 필터 해상도 + 1

만약 입출력 해상도를 유지하고 싶다면  ?

→ 출력값 중 정의되지 않은 경우, 0 혹은 가장 가까운 출력 값으로 대체 (패딩)

### 1-2. 모델 구조

CNN은 이미지의 로컬 정보를 잘 뽑아냄

### 1-2-1. CNN 모델 구조 - 수용영역 (Receptive Field)

중첩과 수용영역

- CNN 레이어는 이미지의 작은 부분인 ‘지역 정보’를 추출하는데 유리하게 설계
- 이미지 활용하는 다양한 작업에서 이미지 전체 의미 파악이 필요하다 (맥락)
- ‘지역정보 + 이미지 전체의 맥락’을 이해하고 활용하기 위한 설계가 필요함

수용영역

- CNN이 이미지를 처리하면서 한 번에 볼 수 있는 영역의 크기
- 네트워크의 시야
- 일반적으로 네트워크가 깊어질수록 수용영역도 넓어짐 → 폭 넓은 맥락 이해 가능

맥스풀링 

- 정해진 커널 사이즈로 이미지를 나누어 각 영역 내 가장 큰 값을 선택하는 연산
- → 각 영역 내에서 가장 큰 값 선택

### 1-2-2. CNN 모델 구조 - 스트라이드 합성곱

## 2. CNN 기반 모델 변천사

### 2-1. AlexNet

5개의 합성곱 계층과 3개의 완전연결 계층으로 구성된 8계층 CNN 모델 

- 메모리 사용은 초기 레이어에 집중
- 모델 상수는 연결형 레이어에 집중
- FLOPs 합성곱에서 주로 발생

### 2-2. VGGNet

5개의 합성곱 블록 + 맥스 풀링 구조

VGG의 레슨

- 작고 단순한 필터를 깊게 쌓으면 성능 올라감
- → 새로운 설계 철학 제시

- AlexNet에서는 레이어별 연산량 차이가 매우 큼
- VGG는 해상도를 줄일 때 마다 채널을 늘려 레이어별 연산량 차이를 줄임

단순하고 깊에 모델을 쌓을 수 있어서 

각각 레이어에 대한 의미에 대해 이야기 할 수 있게 됨 

설계가 단순해서 해석도 쉬워짐 

풀링 이전에 뽑앗던 feature 들이 의미있어짐 

### 2-3. ResNet

합성곱 블록 (VGG와 유사)과 잔차 블록

작은 모델이었을 때의 성능을 보장해줌 

왜 보틀넥 잔차를 사용하는가??

더 깊은 모델을, 더 적은 연산으로 달성할 수 있다 

### 2-4. MobileNet

이후 모델에 영향력

- 가볍고 빠른 모델로 엣지 디바이스에서 딥러닝 모델 실시간 실행 가능함을 보임
- 효율형 모델 구조의 표준 제시 : 깊이별 합성곱과 보틀넥 구조가 이후 경량 모델에서 지속적으로 채택

---

### 강의 정리

---

# 딥러닝 및 이미지 모델 - 다양한 신경망 모델

## 1. CNN의 한계

### 1-1. CNN의 구조 및 장점

### 1-2. 한계

## 2. 시퀀스 데이터 처리 : RNN

### 2-1. RNN (Recurrent Neural Nets)

아크탄젠트 - 비선형에서는 유용하게 쓰임 (사실 많이 쓰지는 않음) 

W 학습 : 역전파 시 모든 은닉 상태에 발생한 기울기를 더하여 연산

### 2-2. 단순 RNN 한계

### 2-3. 대안 모델

기울기 소실을 해결할 ,,, 

## 3. 긴거리 의존성 : 어텐션/ViT

### 3-1. 어텐션 메커니즘

### 3-2. 자기 어텐션 (self-Attention)

### 3-3. 모델 별 특성 정리

### 3-4. ViT 위치 인코딩

**Vit vs ResNet**

**ViT 학습 시 유의점** 

- 학습 기술이 중요하다
    - 막대한 상수를 보유
    - 정규화 기법/데이터 증강 기술이 매우 중요하다
    - 증류학습 방식이 효과적이다
        - 이미 기학습된 모델이 있따 (선생님모델)
        - 학생모델은 선생님 모델 예측을 따라간다
        - 정답도 함께 배우도록 학습한다.

- ViT 모델은 학습 가능한 토큰을 추가로 입력받아. 선생님 CNN의 logit 출력과 동일하게 예측하도록 학습된다.

- 상당한 개선 효과도 확인했다.
- 오래학습하고
- 고해상도로 입력했을 때
- 성능이 더 오른다

**Vision Transformer의 트렌드**

Swin 계열은 특히 검출/분할과 같은 화소별, 공간 이해에서 좋은 성능을 보임 

**ViT 장단점**

---

강의 요약

---

# 딥러닝 및 이미지 모델 - 이미지 모델 학습 전략

# 1. 학습 전략의 중요성

좋은 모델 구조 = 우수한 성능인가 ?

해결하기 위해서 ..

- 구조 설계만큼 중요한 훈련 전략 제시
    - 학습률 스케줄링, 정규화, 초기화, 데이터 전처리 등
- 적용 시 기대효과
    - 일반화 성능 + 학습 효율성 확보

**좋은 구조만으로는 충분하지 않다. → 훈련 전략이 필요하다 !!**

## 2. 모델 구성 : 악마는 디테일에 ..

### 2-1. 활성화 함수

입력 신호의 총합을 출력 신호로 변환하는 함수

- 예시 )
    - 뉴런이 켜질지/ 얼마나 반응할지 를 결정하는 스위치
- 역할
    - 신경망에 비선형성 부여 → 복잡한 패턴 학습 가능
        - 단순 패턴은 선형 분류가 가능하지만 ,복잡한 패턴은 직선만으로 분류가 불가능
        - 활성화 함수를 통해 비선형성을 부여함으로써 복잡한 패턴 학습을 가능하게 함


### 대표 활성화 함수 : Sigmoid

### 대표 활성화 함수 : Tanh

### 대표 활성화 함수 : ReLU

### 대표 활성화 함수 : Leaky ReLU

### 대표 활성화 함수 :  ELU

### 다양한 활성화 함수

### 결론은 ,,

큰 고민하지 말고 ReLU부터 시도하자 .

### 2-2. 데이터 전처리

### 데이터 형식 통일

### 모델별 전처리 방식 예시

### 2-3. 모델 상수 초기화

**아이디어 1** 

- 0으로 초기화

모든 가중치(W)와 편향 (b)을 0으로 초기화하면 무슨 일이 생길까

**아이디어 2** 

- 랜덤 초기화

작은 랜덤 숫자 (0.01 곱 → 0.01 분산)

: Gaussian 분포를 따른다. 0을 중심으로

**아이디어 3**

- 자비에(Xavier) 초기화

가중치 초기 분포의 분산을 입력 차원으로 맞춤 

- std = 1/sqrt(Din)

ReLU와 궁합이 좋지 않아 허 초기화 라는 아이디어가 등장했다.

**아이디어 4**

- 허(He) 초기화

가중치 초기 분포의 분산을 2*입력 차원으로 맞춤

- std = sqrt(2/Din)


### ResNet 의 등장 배경

주의 사항

### 2-4. 모델 정규화

과정합을 방지하는 방법들. ..

**아이디어 1**

- 가중치 감소

학습 과정에서 특정 가중치가 지나치게 커지는 현상을 방지 

**아이디어 2**

- 드롭 아웃

## 3. 학습 안정성 전략

### 3-1. 학습 비율 조정

**학습 비율을 어떻게 선정해야할까?**

- 기본적으로 사용하는 전략
    - 학습 비율을 큰 값에서 시작하여, epoch 지날 수록 작은 값으로 조정함
    - 학습 비율을 줄이는 다양한 전략이 있음 ….
    - Good learning rate의 경우, Loss가 안정적으로 줄어드는 모습을 볼 수 있음

### 학습률 계단식 변경

일정 에폭이 지날 때마다 계단식으로 줄이는 방식

- 에폭 : 학습 데이터를 1번 다 외움을 의미
- K 에폭마다, 감소 계수 * 초기 학습률 만큼 감소하는 전략

### 학습률 코사인 변경

- 계단형 방식은 변경 지점을 여러개 선정, 복잡하다는 단점이 있음
    - 학습 곡선이 불안정할 수 있음
- 코사인 파형에 따른 변경
    - 학습률을 코사인 함수 곡선처럼 점점 줄여가는 방식
    - 직관 : ‘처음엔 크게, 나중엔 미세하게 조율’

### 학습률 선형 변경

- 직선을 따라 변경

### 그 밖의 변경

- 다양한 감소 방식이 있다

### 학습 종료

- 과적합 방지를 위한 또 다른 방법

### 3-2. 하이퍼파라미터 선정

**파라미터**

- 학습을 통해 모델 스스로 얻는 값
    - 예 ) 뉴럴 네트워크의 가중치, 편향 등
- 하이퍼 파라미터
    - 학습 전에 사용자가 정해야 하는 값
    - 학습이 진행되는 동안 고정됨

모든 조합을 실험하는 것은 비추

### 체크리스트

1. **데이터 입력 등 문제가 없을까?**
    - 초기 손실값을 확인하자.
        - 데이터 손실을 측정할 때, 분류 문제 등에서 많ㅇ ㅣ사용되는 손실 함수인 CE 손실이라면
        - 초기 손실은 log(C)와 유사한 수준이어야 정상임
        - 만약 예상치를 크게 벗어난다면 데이터 로딩/라벨 문제를 확인해야함
2. **학습률과 초기값 확인**
    - 작은 샘플을 활용, 일부러 과적합 시켜보자
        - 어떻게? 정확도 100% 달성까지 작은 학습 샘플에 대해 학습 진행
        - 정규화 x , 고의로 과적합을 야기
        - 손실이 줄지 않는다면 ?
            - 학습률 초기화 문제
            - 모델 구조의 오류 가능성
            - 활성화 함수 이슈
            - 최적화 툴 이슈/코드 버그
        - 손실이 폭발한다면?
            - 학습률을 줄이고, 초기화 변경 (가장 흔한 이슈)
            - 네트워크 깊이가 갚거나, 구조적 이슈 (RNN)
            

오버핏이 0인 건 모두 학습했다는 뜻임

1. **체크리스트 2 에서 얻은 구조, 최적화 툴 활용, 학습률을 찾자.**
    - 모든 학습 데이터 활용 , 작은 iteration (~100) 을 빠르게 손실이 줄어드는 학습률을 우선 탐색함
        - 경향을 빠르게 파악
    

1. **체크리스트 3에서 얻은 후보 학습률 + 후보 가중치 변형 방식 중 , 좋은 조합을 탐색** 
    - 상기 조합을 활용, 1-5 에폭 동안 모델을 학습시켜 성능 비교

초반에 아니면 떡잎부터 아니다.

1 ~ 5 만큼만 해도 가능성이 보여야함 

1. **체크리스트 4에서 얻은 좋은 조합들을 활용, 10 - 20 에폭까지 추가 학습**
    - 학습률 변경(줄이기)는 적용하지 않음

1. **학습률에 따른 손실곡선 관찰** 

1. 초반 손실값이 유지되는 경우
    1. 초기값이 문제이다
2. 손실이 크게 떨어지지 않고 유지되는 경우
    1. 학습률 변경 (줄이기) 시도하기 - 스케줄링
3. 학습률 변경을 적용하였는데, 이후 학습이 진행되지 않는 경우
    1. 정체가 오기 전, 너무 일찍 학습률을 줄였다고 판단
4. 정확도가 증가할 경우
    1. 계속 학습할 것
5. 학습/검증 정확도가 크게 벌어진 경우
    1. 과적합된 것 ! 정규화 방식을 도입해야함
6. 학습데이터/검증데이터 결과의 차이가 매우 적다면?
    1. 모델의 힘이 너무 약한 것 - 큰 모델 도입하여 더 오래 학습해야함
