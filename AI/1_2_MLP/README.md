MLP - Multi Layer Perceptron - 여러층 뉴런

AI : 인간 흉내

ML : 모델을 만드는 분야

ㄴ DeepLearning : 머신러닝의 하위분야 / 인공신경망 구조를 사용해 ML을 하는 분야 (ML의 그래프를 구하기 위해 필요한 파라미터값을 여러개 저장)

## 워드 임베딩과 순환신경망 기반 모델

## 1. 워드 임베딩

### 1-1. 원-핫 인코딩

- 규칙 기반 혹은 통계적 자연어처리 연구의 대다수는 단어를 원자적 기호로 취급한다.
    - hotel, conference, walk ..
- 벡터 공간 관점엣거 보면, 이는 한 원소만 1이고 나머지 모두 0인 벡터를 의미

**원 핫 인코딩의 문제점**

- 0이 아닌 곳만 의미가 있는 벡터이다.
- 원-핫 인코딩과 같은 전통적인 텍스트 표현 방식에는 여러 한계가 존재함
- 검색 쿼리 벡터와 대상이 되는 문서 벡터들이 서로 직교하게 되어, 원-핫 벡터로는 유사도를 측정할 수 없다.
    1. 차원의 저주
        - 고차원의 희소 벡터를 다루기 위해선 많은 메모리가 필요하다.
        - 차원이 커질수록 데이터가 점점 더 희소해져 활용이 어렵다
    2. 의미적 정보 부족
        - 비슷한 단어라도 유사한 벡터로 표현되지 않는다.
        - ex ) ‘은행’과 ‘금융’은 의미적으로 밀접하지만, 원 - 핫 인코딩에서는 전혀 무관한 벡터로 취급됨

### 1-2. 워드 임베딩

**주변 단어 활용**

- 단어를 주변 단어들로 표현하면 많은 의미를 담을 수 있다.
- 현대 통계적 자연어처리에서의 가장 성공적인 아이디어 중 하나이다.


**워드 임베딩이란 ?**

- 단어를 단어들 사이의 의미적 관계를 포착할 수 있는 밀집되고 연속적/분산적 벡터 표현으로 나타내는 방법이다.
    - 원-핫 인코딩에선 은행과 금융이 완전히 독립적인 무관한 벡터로 표현되었지만, 워드 임베딩에선 두 단어의 벡터가 공간상 서로 가깝게 위치하며 이를 통해 의미적 유사성을 반영할 수 있다.


- 대표적인 워드 임베딩 기법 - Word2Vec
    - google에서 개발한 워드 임베딩 기법
    - 단어의 표현을 간단한 인공 신경망을 이용해서 학습한다.
    - 사용 알고리즘
        - skip=grams (SG) 방식
            - 중심 단어를 통해 주변 단어들을 예측하는 방법이다.
            - 단어의 위치 앞 / 뒤에 크게 구애 받지 않는다.
        - continuous bag of words (CBOW) 방식
            - 주변 단어들을 통해 중심 단어를 예측하는 방법
            - 문맥 단어들의 집합으로 중심 단어를 맞춘다

**skip-grams**

**countinuous bag of words** 

**skip-gram vs cbow**


## 2. 순차적 데이터

시퀀스 데이터 (단어의 연속인 문장 ) 같은 것은 임베딩된 모델로 처리할 수 없다.

- 순차적 데이터란, 데이터가 입력되는 순서와 이 순서를 통해 입력되는 데이터들 사이의 관계가 중요한 데이터이다.

**순차적 데이터의 특징**

1. 순서가 중요
    1. 순서를 바꾸면 의미가 달라지니까
2. 장기 의존성
    1. 멀리 떨어진 과거의 정보가 현재 / 미래에 영향을 준다
3. 가변 길이
    1. 순차 데이터는 길이가 일정하지 않고 단어 수도 제각각임

**순차적 데이터를 처리하려면?** 

- 일반적인 모델들 (선형회귀 , MLP로는 불가능)
- 스퀀스 모델이 필요하다 (RNN, LSTM, Transformer 등)

## 3. RNN

전통적인 인공신경망 

- 고정된 길이의 입력을 받아 가변 길이의 데이터를 처리하기에 적합하지 않다.

하지만 RNN은

가변 길이의 입력을 받을 수 있고, 이전 입력을 기억할 수 있기 때문에, 순차적 데이터 처리에 적합한 아키텍처이다. 

RNN 아키텍처 설명

- 전통적인 신경망과 달리 RNN은 이전 시점의 정보를 담는 hidden state를 가지고 있다.
- 따라서, 입력 시퀀스 벡터 x를 처리할 때 각 시점마다 recurrence수식을 적용하여 hidden state를 업데이트 한다.


새로운 우리의 이해를 반복적으로 만들어냄 

초기값 (h0) → h0 + x1 = h1 → h1 + x2 = h2 …

T가 얼마든 간에 Wxh를 반복해서 사용한다.

### RNN의 특징

- RNN은 **한 번에 하나의 요소를 처리**하고 정보를 앞으로 전달한다.
- 펼쳐서 보면 RNN은 각 측의 하나의 시점을 나타내는 깊은 신경망처럼 보인다
- RNN hidden state 를 유지하면서 가변 길이 데이터를 처리할 수 있다.
- RNN의 출력은 **과거 입력에 영향을 받는다**는 점에서, feedforward 신경망과 다르다.

### RNN의 한계 - 기울기 소실 문제 (vanishing gradient)문제

- 기울기 소실 문제란?
    - 딥러닝에서 역전파 시 앞쪽 층의 기울기가 0에 가까워져서 장기 의존성 학습이 어려워지는 현상
- 왜 일어날까?
    - 역전파 과정에서 작은 값들이 계속 곱해진다.
    - 과거 시점에서 온 오차 신호는 갈수록 더 작은 기울기를 갖게 된다.
    - 결국 파라미터들이 작기 의존성은 학습하지 못하고, 단기 의존성만 포착하게 된다.
        - 우리가 100페이지 짜리 책을 읽으면서 앞쪽내용을까먹는거랑 비슷….

## 4. LSTM

### LSTMs

- 기울기 소실 문제를 해결하기 위해 제안된 RNN의 종류
- LSTMs의 특징
    - 시점 t에서 RNN은 길이가 n인 벡터 hidden state ht와 cell state ct를 가진다.
        - hidden state 는 short-term-information을 저장함
        - cell state 는 long-term-information을 저장한다.
    - LSTMs는 cell state에서 정보를 **읽고 지우고 기록**할 수 있다.

내 머리가 RNN 으로 되어있다면?

강사님의 첫사랑 이야기를 (수업과 관련된 이야기)를 걸러낼 수 없다. 정보를 거부하는 능력이 없기 때문에

input 

forget gate & input gate

new cell content & output gate

## 자연어 생성 모델(Seq2Seq, Attention)

목표

- 언어 모델의 개념과 역할을 설명할 수 있다.
- 언어 모델이 문맥을 바탕으로 단어의 확률을 예측하는 방식을 이해한다
- seq2seq 구조의 기본 아이디어(인코더-디코더)를 설명할 수 있다
- attention 메커니즘의 개념과 필요성을 설명할 수 있다.

## 1. 언어모델이란?

- 언어모델이란 인간의 두뇌가 자연어를 생성하는 능력을 모방한 모델이다.
    - 단어 시퀀스 전체에 확률을 부여하여 문장의 자연스러움을 측정한다
    
- 한 문장의 확률은 각 단어의 조건부 확률들의 곱으로 표현할 수 있다.

- 폰에서 키보드에 글자를 작성할 때 단어 추천 해주는 것 = 일상 속 언어 모델

**대표적인 언어모델 - N-gram 언어모델**

SMT의 한계

- 구조적 복잡성
- 많은 수작업
- 언어별 자원 구축 필요
- → 유지 및 확장에 어려움 존재

이런 한계로 인해 이후의 기계 번역 연구는 NMT로 넘어가게 되었다.,

## 2. Seq2Seq은 어떻게 작동할까?

### Neural Machgine Translation 이란 ?

- 인공 신경망을 이용해 기계 번역을 수행하는 방법이다.
- 이때 사용되는 신경망 구조를 s2s라고 하며 두 개의 RNNs로 이루어진다.

**translation이 어려운 이유** 

- 번역 문제는 입력과 출력의 길이가 다른 수 잇다.
    - 영어 : the black cat drank milk
    - 프랑스어 : le chat noir a bu du lait

→ 따라서, NMT에서는 길이가 다른 시퀀스 간의 매핑을 처리할 수 있어야 한다.

**Seq2Seq의 아이디어**

- 2개의 LSTM을 이용하자
    - 한 LSTM은 입력 시퀀스를 한 타임스텝씩 읽고 고정된 차원의 큰 벡터 표현을 얻기 (인코더)
    - 다른 LSTM은 앞에서 얻은 벡터로부터 출력 시퀀스를 생성하기 (디코더)

### 2-1. 인코더-디코더 구조

- seq2seq는 encoder decoder로 이루어진다
    - 인코더는 입력 문장에 담긴 정보를 인코딩한다
    - 디코더는 인코딩된 정보를 조건으로 하여 타겟 문장(출력)을 생성한다.

### **Seq2Seq의 다양한 적용**

- 기계번역 외에도 다양한 태스크에 적용할 수 있다.
    - **요약** : 긴 길이의 문서를 읽고 , 짧은 길이의 문장으로 요약된 텍스트를 출력하는 테스크
    - **대화** : 사용자의 발화를 기반으로 맥락에 맞는 대답 (출력 텍스트)를 생성하는 태스크
    - **코드 생성** : 자연어로 작성된 설명 혹은 명령어를 입력 받아 , 그에 대응하는 프로그래밍 코드 혹은 쿼리를 출력하는 태스크

### **Seq2Seq 학습 수행**

- seq2seq 모델은 인코더와 디코더가 하나의 통합 네트워크로 연결되어 있다.
- 디코더에서 발생한 오차는 역전파 과정을 통해 입력을 처리한 이코더까지 전달되어 전체 네트워크가 ent-to-end로 동시에 최적화된다.


- 학습 초반에는 모델의 예측 능력이 떨어지기 때문에 학습이 불안정할 수 있음
- Teacher Forcing
    - 모델이 스스로 예측한 단어 대신 정답 단어를 디코더 입력으로 강제로 넣어줌으로써 훨씬 안정적이고 빠르게 학습을 수행하는 방법

### **Seq2Seq의 토큰 출력 방법**

- 토큰을 출력하는 방법 중 하나로, 각 단계에서 가장 확률이 높은 단어를 선택함
- 한계
    - 되돌리기 불가능


- **Beam Search**
    - 빔처럼 쭉 나아가며 찾아서 빔서치 ㅋㅋ
    1. 매 단계마다 k개의 가장 유망한 후보 유지
    2. 후보가 EOS에 도달하면 완성된 문장으로 리스트 추가
    3. EOS문장이 충분히 모이면 탐색 종료
    4. 각 후보들의 점수를 로그 확률의 합으로 구해 최종 선택 

## 3. Attention

### **Seq2Seq의 한계 : the bottleneck problem**

- Bottleneck problem
    - 인코더는 입력 문장 전체를 하나의 벡터로 요약하는데 ,마지막 hidden state에 문장의 모든 의미 정보가 담긴다.
    - 고정 길이 벡터 하나에 모든 문장의 의미를 압축하다 보니 정보 손실이 생길 수 있는데, 이를 bottleeneck problem 이라고 한다.

### Attenteion의 인사이트

- 디코더가 단어를 생성할 때 인코더 전체 hidden state  중  필요한 부분을 직접 참조할 수 있도록 한다.
- 즉 매 타임스텝마다 어떤 단어/구절에 집중할지를 가중치로 계산해 bottleneck 문제를 완화함

인코더에 정보가 많긴 하지만 진자 중요하게 봐야할 것은 무엇인가.

### Attention의 효과

Attention mechanism은 많은 정점이 존재한다.

1. NMT 성능 향상
    - 디코더가 소스 문장 전체가 아닌, 필요한 부분에만 집중할 수 있기 때문이다.
2. Bottleneck Problem 해결
    - 디코더가 인코더의 모든 hidden states에 직접 접근할 수 있다.
3. vanishing Gradient Problem 완화
    - Attention 은 멀리 떨어진 단어도 직접 연결할 수 있게 해준다.

- 가로축: 문장 길이
- 세로축: BLEU 점수
- 범례: RNNsearch-50, RNNsearch-30, RNNenc-50, RNNenc-30

그림 2: 테스트 셋에서 생성된 번역의 BLEU 점수를 문장 길이에 따라 나타낸 것이다. 결과는 모델이 모르는 단어를 포함한 문장까지 모두 포함한 전체 테스트 셋에 대한 것이다.

해석 가능성

- attention 분포를 보면 decoder 가 어떤 단어를 생성할 때, 입력 문장의 어느 부분에 집중했는지 확인 할 수 있다.
    - 모델의 의사결정 과정을 해석할 수 있는 단서

정렬

- 기계번역에서는 전통적으로 단어 alignment 모델을 따로 학습해야 했다.
- 하지만 attention을 통해 decoder가 필요한 입력 단어에 자동으로 집중하기 ㄸㅐ문에 , 단어와 단어 간의 매핑 관계를 자연스럽게 학습한다.

## Transformer

## 1. Self-Attention

### 1-1. RNN이 꼭 필요할까?

RNN이 하던 정보전달을 Attention이 더 효율적으로 수행할 수 있다면, 굳이 recurrence가 필요할까?

### 1-2. RNN의 한계점

1. 장기 의존성
2. 병렬

**그렇다면 Attention은 ?**

### 1-3. Self - Attention의 한계

1. 순서 정보 부재
    - 단어 간 유사도만 계산하기 때문에 ,단어의 순서를 고려하지 않는다.
2. 비선형성 부족
    - attention 계산은 본질적으로 가중 평균 연산이라는 선형 결합에 불과하기 때문에, 복잡한 패턴이나 깊은 표현력을 담기 어렵다
3. 미래 참조 문제
    - 언어 모델은 시퀀스를 왼족에서 오른쪽으로 생성해야 하지만, self-attention은 모든 단어를 동시에 보기 때문에, 아직 생성되지 않아야 할 미래 단어를 참조한다.

### 1-4. 한계 해결 - Positional Encoding

- 순서 정보 부재 문제를 해결하기 위해 사용
- 각 단어 위치를 나타내는 위치 벡터를 정의해 , 단어 임베딩 값에 더해 최종 입력으로 사용한다.

1. sinusoidal position encoding
    - 서로 다른 주기의 사인/코사인 함수를 합성해 위치 벡터를 만드는 방법
2. Learned Absolute Position Embedding
    - 위치 벡터 모두 학습 파라미터로 설정해 학습 과정에서 데이터에 맞춰 최적화하는 방법

### 1-5. 한계 해결 - Feed-Forward Network 추가하기

- self-attention 연산은 비선형 변환이 없어, 복잡한 패턴 학습에 한계가 존재한다.
- 각 단어 출력 벡터에 feed-forward network (Fully Connectedd + ReLU 등)을 추가해 self-attention이 만든 표현을 깊고 비선형적인 표현으로 확장한다.

### 1-6. 한계 해결 - Masked Self-Attention

- 단어를 생성할 때는 한 단어씩 순차적으로 미래 단어를 예측해야 하지만, self-attention은 기본적으로 모든 단어를(미래 포함) 동시에 참조한다.
- attention score를 계산할 때 미래 단어에 해당하는 항목을 -무한대 로 설정해 , 계산을 수행할 때 반영되지 않도록 한다.

### 1-7. 정리

- Self-Attention은 문장 내 모든 단어가 서로 직접 상호작용하여
    1. 장거리 의존성을 효율적으로 포착
    2. 병렬 처리를 가능하게 

## 2. Transformer

### 2-1. Attention is all you need

- self-attention을 핵심 메커니즘으로 하는 신경망 구조

### 2-2. Transformer

- Transformer는 인코더-디코더 구조로 설계된 신경망 모델이다.
    - 인코더 : 입력 문장을 받아 의미적 표현으로 변환을 수행한다
    - 디코더 : 인코더의 표현과 지금까지 생성한 단어들을 입력 받아, 다음 단어를 예측한다.

→ 이 중 디코더가 언어 모델과 같은 방식으로 동작한다. 

쿼리 키 밸류 값도 자연스럽게 커지기 때문에 header 수 만큼 나눈다.

## 1. 사전학습

### 1-1. 사전학습이란?

- 파운데이션 모델을 만드는 것
- 사전학습이란 대규모 데이터 셋을 이용해 모델이 데이터의 일반적인 특징과 표현을 학습하도록 하는 과정
- 특히 언어 모델은 인터넷의 방대한 텍스트를 활용해 비지도학습 방식으로 학습되어 일반적인 언어 패턴 지식 문맥 이해 능력을 습득한다

인코더 BERT / 인코더 + 디코더 / 디코더 - gpt

## 2. Encoder 모델

BERT - google 에서 공개한 transformer 기반의 모델로, Masked LM 방법으로 사전학습을 수행했다. 

에러에 강하게 만들기 위해서 pizza 같은 거 넣고 went 로 복구 시키기도 함 


**[요즘엔 잘 안 쓰지만 읽어볼 것 - Next Sentence Prediction (NSP) ]**

문서의 어디가 정답인지 찾음 

## 3. [인코더 디코더 모델] : 잘 안 씀

선택과 집중 

T5

## 4. Decoder 모델

GPT 1 은 분류형 모델로 주로 사용함

GPT-2 에서 챗봇이 슬금 등장  

예전엔 파라미터가 변경되는게 learning 이었는데 

이제는 모델이 변경되는 것을 맥락상에서 깨달으면 learning ⇒ in - Context Learning

few-shot chain of thought

think step by step 

추론만을 위한 모델 o .. 는 CoT 알고리즘 내장 

단순하게 명령을 던지는 것만으로는 문제를 해결하지 못할 때가 올 것이다 ..

프롬프트 엔지니어링 : 
효율적인 프롬프트 작성

컨텍스트 엔지니어링 :
프롬프트 내용이 점점 길어지니까 어떻게 하면 효율적으로 줄일지 고민함 
