## 1. 선형회귀

### 1-1. 선형회귀란

- 입력 변수와 출력 변수 사이의 관계를 직선 형태로 근사하여 예측하는 통계적 방법이다.
- 지도학습의 가장 기초가 되는 접근 중 하나
- 선형회귀는 개념적으로도, 실무적으로도 매우 유용하다.

→ 파란색 실선 - 선형 회귀의 결과

### 1-2. 선형회귀를 통해 대답할 수 있는 질문들(광고 데이터 예시)

- 광고비와 매출 사이에 관계가 있는가?
- 그 관계의 강도는 어느 정도인가?
- 어떤 매체가 매출에 기여하는가?
- 미래 매출을 얼마나 정확히 예측할 수 있는가?
- 매체 간에 상호작용(시너지)이 있는가?

## 2. 단순 선형 회귀

### 2-1. 단순선형회귀란

- 한 개의 설명변수와 하나의 반응변수 사이의 선형 관계를 찾는 방법
- 가장 잘 맞는 직선을 찾아 예측에 활용하는 것이 목표

- 모형 가정: Y = β₀ + β₁ X + ε
    - β₀: 절편 (X = 0일 때 Y 값)
    - β₁: 기울기 (X가 1 단위 증가할 때 Y의 평균 증가량)
    - ε: 관측 오차

- hat(예, ŷ, β̂) 표기는 추정값을 의미

### 2-2. 최소제곱법

- **실제 관측값과 예측값의 차이**(잔차)를 제곱해 합한 값(RSS, **잔차제곱합**)을 최소화하는 방법


**잔차**

- eᵢ = yᵢ − ŷᵢ  (예측값 ŷᵢ = β̂₀ + β̂₁ xᵢ)

**RSS(잔차제곱합) 정의**

- RSS = e₁² + e₂² + … + eₙ²
- 다른 표현: RSS = Σᵢ₌₁ⁿ eᵢ² = Σᵢ₌₁ⁿ (yᵢ − ŷᵢ)² = Σᵢ₌₁ⁿ (yᵢ − β̂₀ − β̂₁ xᵢ)²

RSS를 작게 만드는 것이 우리의 학습 목표.

ŷᵢ 가 yᵢ 에 가깝길 바라기 때문에 → 가까워지면 eᵢ 가 작아지고 → RSS는 제곱의 합이기 때문에 작을 수 밖에 없음


절편 : RSS를 최소화하는 값을 구할 수 있음 

### 2-3. 단순선형회귀 : 광고 데이터

- TV광고비(x)와 제품 판매량(y)의 선형 관계 예측
- 단순 선형 회귀를 적용하여, 각 데이터에서 잔차제곱을 가장 작게 만드는 직성(최소제곱법)이 선택됨

- 선형회귀는 절편과 기울기만 있으면 직선을 찾을 수 있음
- 빨간점에서 파란선까지의 회색선 = 잔차 (실제 관측값과 예측값의 차이)
- ŷᵢ (와이 아이 햇) : 예측값 → 모두 파란선에 위치함

yᵢ : 빨간점 (실제 관측 데이터)

ŷᵢ : 예측값 (파란직선)

단순히 숫자만 보는 것이 아니라 coefficient / Std.Error 의 값을 봐야함 (t-statistic)

## 3. 다중선형회귀

### 3-1. 다중선형회귀란?

- Tv 광고, 라디오 광고, 가격 계절 등 복수 요인을 함꼐 고려하여 매출을 설명
- vs 단순 선형 회귀 : tv광고 → 매출 한 가지 관계만 고려함

- 독립 변수(설명 변수, feature)가 여러 개 존재할 때 사용하는 회귀 분석 기법
- 단순 선형 회귀는 하나의 변수만 고려하지만, 다중 선형 회귀는 여러 독립 변수를 동시에 고려하여 종속 변수와의 관계를 구함

### 3-2. 다중선형회귀의 추정과 예측


### 3-3. 다중선형회귀 계수 추정 유도


### 3-4. 다중선형회귀 결과 - 시각화


### 3-5. 다중선형회귀 결과 : 광고 데이터

- 신문 광고는 하나마나 매출에 연관이 없다
- 결정계수 0.897 : 모델 설명력이 매우 높음 → 단순선형회귀 결과와 비교하였을 때 향상된 예측
- 티비가 라디오 보다 영향을 더 많이 주는 것은 ?
    - 선형성이 더 큰 것 (t-statistic)
- 단위당 매출의 증가량이 높은 것은 ?
    - 기울기가 더 큰 것

## 4. 선형회귀 주의사항

### 4-1. 선형회귀 결과 검증 및 테스트 성능

**훈련 데이터에서의 성능**

- 회귀식을 만들 때 최소 제곱 해는 훈련 데이터만 보고 계산됨
- 학습에 사용된 훈련 데이터에서는 적합이 잘 되어 있을 것임
- 그러나 이것은 테스트 성능을 과소평가할 가능성이 높음

**테스트 성능 평가 필요**

- 선형회귀도 변 일반화 성능을 확인하려면 훈련에 사용되지 않은 새로운 데이터에 적용해봐야함.
- 수가 많거나 고차항을 사용하면 과적합(overfitting) 문제가 여전히 발생할 수 있음
- 교차검증/검증 등을 통해 적절한 적합을 찾을 수 있음

### 4-2. 다중선형회귀시 회귀 계수 해석의 주의점

- 이상적 상황
    - 변수들이 연관되지 않고 독립적일 때 → 계수 해석이 명확함
- 문제 상황
    - 변수들이 서로 연관되어 있다면 → 계수 추정이 불안정해지고 해석에 혼동이 발생할 수 있음
- 관찰 데이터의 상관관계로 인과 관계를 주장해서는 안됨
→ 선형회귀는 그냥 두개의 상관관계를 알아보기 위함 


---

## 로지스틱 회귀

## 1. 분류

### 1-1. 분류란?

- 정해진 범주(카테고리) 중 하나로 지정하는 것
- 범주형 변수 : 수치의 크고 작음이 아니라 유한한 번부(성별, 혈액형, 지역 등)으로 표현하는 변수

분류 함수의 목표

- 분류 함수 f(x) 를 학습하여 입력 x가 속할 범주(카테고리)를 예측
- 범주의 직접 예측보다 각 범주에 속할 확률 P를 추정하는 것이 더 유용할 때가 많음

### 1-3. 분류 문제에 선형회귀를 써도 될까?

- 선형회귀는 분류 문제에 사용하기에 부적절하다!
    - 선형 함수를 계산하는 문제로 예측 값이 제한된 값을 갖게 못함.
    - 따라서 선형회귀는 **예측 확률이 0보다 작거나 1보다 크게 예측** 될 수 있어 확률로 쓰기 부적절함

### 1-4. 분류 문제에서 적합한 모델

**로지스틱 회귀**

- 시그모이드 함수를 활용해 0 ~ 1 범위 내 확률값 예측 보장
- 순서가 없는 범주를 확률로 직접 예측하는 적절한 분류 방법

## 2. 로지스틱 회귀

### 2-1. 로지스틱 회귀의 모형식

함수의 출력 범위가 모든 입력에 대해 0~1 사이의 범위를 가지는 함수를 활용하자

### 2-2. 로지스틱 회귀 모형 vs 선형 모형


**오즈**

- 성공 (y=1) 확률이 실패 (y=0)확률에 비해 몇배 더 높은가를 나타낸다.
- 이길 확률 / 질확률 | 성공 확률 / 실패 확률

**로짓변환 (logit)**

- 로짓변환은 오즈에 로그를 취한 함수형태


### 2-3. MLE 활용 모수 추정

선형 회귀 

- RSS를 minimize 하는 베타값(기울기, 절편)을 찾는 것 (목적)

- 현재 함수가 데이터와 오차가 작은지를 평가하기 위해 평균 제곱 오차(MSE)를 지표로 삼았었음

**로지스틱에서는 우도**를 지표로 삼음

**우도(likelihood)**

- **현재 확률 함수가 현재 관측하고 있는 데이터를 얼마나 잘 설명하고 있는지**를 나타낸 지표

모델의 학습은 우도 값을 높여 최대화가 되도록 하는 것이 목표이며 이를 MLE라고 한다. 


우도가 최대 값을 나올 수 있는 베타 찾기 

→ 양변에 로그를 취해 곱셈을 더하기로 변환시킨 뒤 log-likelihood를 만들어 최대화 

→ 로그로 변환하여 문제를 접근해도 괜찮을까?

- 괜찮음 ㅇㅇ
    - log-likelihood를 최대화하는 beta값이랑
    - likelihood를 최대화하는 beta값이랑 같기 때문에

### 2-4. 로지스틱 회귀 결과


- balance가 1만큼 증가할 때 연체할 확률이 0.0055만큼 증가 ? NO
- balance가 1만큼 증가할 때 log-odds 가 0.0055 만큼 증가. Y

---

## 1. 모수적 함수로서의 선형 모델

### 1-1. 단순(1D) 선형모델: 모수적 함수

## 2. shallow 네트워크

### 2-1. shallow 네트워크 vs 1D 선형회귀

### 2-2. shallow 네트워크: 활성화 함수

### 2-3. shallow 네트워크: 모수(parameter)

### 2-4. shallow 네트워크 : piecewise linear 함수

### 2-5. shallow 네트워크 : Hidden Units

### 2-6. shallow 네트워크 : 각 단계별 계산

- 세개의 액티베이션 함수 (히든 유닛) 이 합쳐져서 꺾은선이 3개 생김

### 2-7. 네트워크 도식화

- 레이어가 한개(h 1,2,3 묶음)인 게 shallow 네트워크

## 3. shallow 네트워크의 표현력

### 3-1. 더 많은 Hidden Unit 가능


### 3-2. Hidden Unit을 많이 두면

임의의 1차원 함수를 원하는 정확도로 근사할 수 있다.

- 히든 유닛을 충분히 늘리면 원하는 정확도로 항상 맞출 수 있다

### 3-3. 보편적 근사 정리

**Hidden unit을 충분히 많이** 갖는다면, 얕은 신경망은 **임의의 연속 함수를 임의의 정밀도로 근사**할 수 있음 

## 4. 다중 출력 / 입력

### 4-1. 2개 출력의 네트워크

- 히든 유닛이 변경되지 않는 이상 꺾이는 지점이 변경되지는 않음

### 4-3. 2개의 입력 네트워크 : 단계별 계산

- 밝을 수록 높은 곳을 나타내는 평면

- 0이하는 잘래냄

피스와이즈 리니어..?

ReLU 함수를 사용해서 가능했다 →


### 4-5. 임의의 개수의 입력, Hidden Unit, 출력


## 5. Deep 네트워크

### 5-1. 2개의 네트워크를 하나로 합성

- 히든 유닛 3개 1개의 리니어


### 5-4. shallow 네트워크와 Deep 네트워크 비교

2D Input 2개의 네트워크를 하나로 합성

## 6. Deep 네트워크 수식 표현

### 6-1. 2개의 네트워크를 하나로 합성


### 6-3. 2층 네트워크의 표현

### 6-5. 용어의 단순화

### 6-6. 네트워크 도식화


---

## 1. 선형회귀 예시

### 1-1. 손실함수

모델이 얼마나 잘못 예측하는지를 측정하는 함수 

- 값이 작을수록 모델이 더 정확하게 학습되었다는 의미

### 1-2. 학습

손실함수를 최소화하는 파라미터를 찾음

### 1-3. 1D 선형회귀 예

- 선형회귀 모델에서 손실함수는 예측값과 실제값의 차이를 제곱하여 환산한 값
    - 최소제곱 손실함수 MSE
- 학습의 목적은 이 손실함수를 최소화 하는 직선을 찾는 것

### 1-4. 1D 선형회귀 학습

- 등고선(왼쪽)
    - 손실 함수 값의 크기
        - 밝을수록 손실이 큼
        - 어두울수록 손실이 적음
- 데이터와 선형함수 직선(초록색선):
    - 데이터와 선의 오차가 크다면 손실 값이 큼
        - 주황색 점 : 실제 데이터
        - 초록색 직선 : 현재 파라미터로 만든 모델

**경사하강법**

- 손실 함수의 값이 줄어드는 방향으로 파라미터를 이동하는 과정


## 2. 수학 리뷰

### 2-1. 미분을 이용한 최적화

미분을 통한 기울기 이해

- 이차 함수로, 그래프는 위로 열린 포물선
- 미분 결과 (2x-4)는 기울기를 의미
    - 기울기가 0이 되는 지점이 극값 (최소값)

## 3. 경사하강법

### 3-1. 경사 하강 알고리즘

- 경사 하강법은 손실 함수를 최소화하기 위해 파라미터를 반복적으로 갱신하는 알고리즘
- 기울기 계산 : 손실 함수를 파라미터에 대해 편미분 (각 원소에 대한 미분) 진행

**손실 함수의 기울기 계산**

- 경사 하강법의 첫 단계 : 편미분

- 두번째 단계 : 파라미터 업데이

### 3-4. 함수에 따른 최적화 난이도

NeLU → Non-convex

## 4. 확률적 경사 하강법

### 4-1. 경사 하강법 vs 확률적 경사 하강법

경사 하강법의 단점

- None-convex문제에서 지역 최소점에 빠지기 쉬움
- 매 스텝마다 전체 데이터에 대한 미분값을 구하여 업데이트함.
- 스탭별 계산량이 많음

그림

- 점 1, 2, 3 과 경로는 경사 하강법으로 Loss를 줄여가는 과정
- 점 2에서 출발한 경사하강 방ㅂ접은 local 최소점에 빠짐

**대안**

- 전체 데이터를 한 번에 쓰는 대신, 무작위로 선택한 데이트 샘플 사용
    - **확률적 경사 하강법 (SGD)**

### 4-4. 확률적 경사 하강법의 특성

- 무작위 샘플 데이터를 활용한 미분으로 경로의 무작위성이 있음
- Local최소점에서 빠질 위험이 상대적으로 적음

- 국소 최솟값
    - 전체 데이터가 아닌 일부 배치로 기울기를 계산하기 때문에 노이즈가 섞여 있음
    - 노이즈가 오히려 local minima, saddle point에서 빠져나오는 도움이 됨
- 노이즈가 있지만 여전히 타당한 업데이트
    - 미니배치의 기울기는 정확한 전체 기울기가 아니지만, 평균적으로 올바른 방향을 가리킴
    - 학습이 점진적 최적점 방향으로 수렴
- 계산 비용 절감
    - 전체 데이터셋에서 작은 배치만 사용하므로 반복(스텝)당 연산량이 적음
    - 큰 데이터셋에서 효율적으로 학습 가능
- 수렴 특성
    - full batch처럼 매끄럽게 수렴하지 않고 , 무작위성 때문에 더 많이 진동 (지그재그)하면서 움직임
    - 전역 최솟값 근처의 좋은 해에 도달할 수 있음. convex 문제에서는 full-batch 경사하강보다 수렴이 늦을 수 있음

## 5. 역전파 (Backpropagation)

### 5-1. 네트워크 파라미터의 미분

### 5-2. 합성함수의 미분 : 연쇄법칙

### 5-3. 역전파
