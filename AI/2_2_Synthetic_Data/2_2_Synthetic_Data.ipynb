{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UHE4kcvI6sR"
   },
   "source": [
    "### 환경설정\n",
    "- API Key를 환경변수로 관리하도록 설정\n",
    "- Library 설치\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17623,
     "status": "ok",
     "timestamp": 1761008971359,
     "user": {
      "displayName": "[서울_3반_배현지]",
      "userId": "06982767720486100463"
     },
     "user_tz": -540
    },
    "id": "6gMFllZhI2GS",
    "outputId": "ac171015-3430-4494-e39f-f522dabd4dec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# 구글 드라이브를 코랩 환경에 마운트.\n",
    "# 이를 통해 드라이브에 저장된 파일(.env 등)에 접근 가능.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# API 키 파일이 저장된 기본 경로를 설정.\n",
    "base_path = '/content/drive/MyDrive/Colab Notebooks/AI/08_data_argument/'\n",
    "\n",
    "# 폴더 생성 코드\n",
    "# base_path 경로에 해당하는 폴더가 없으면 생성 (-p 옵션).\n",
    "# makedirectory / 위치는 내 구글\n",
    "!mkdir -p \"{base_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XriRM5jdLiNP"
   },
   "outputs": [],
   "source": [
    "# Colab 환경에서 .env 파일을 생성하고 API 키를 저장하는 명령어.\n",
    "# 실제 키를 {your_api_key} 부분에 입력\n",
    "!echo \"UPSTAGE_API_KEY={my_api}\" > \"/content/drive/MyDrive/Colab Notebooks/AI/08_data_argument/.env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1761008972202,
     "user": {
      "displayName": "[서울_3반_배현지]",
      "userId": "06982767720486100463"
     },
     "user_tz": -540
    },
    "id": "iAuR3eW_L7su",
    "outputId": "5f4e281e-89ec-4d59-b96f-991cde5a1e3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success API Key Setting!\n"
     ]
    }
   ],
   "source": [
    "# .env 파일에서 환경 변수를 로드하기 위한 라이브러리.\n",
    "from dotenv import load_dotenv\n",
    "# 운영체제의 환경 변수를 가져오기 위한 함수.\n",
    "from os import getenv\n",
    "\n",
    "# .env 파일을 로드하여 환경 변수를 설정.\n",
    "load_dotenv(base_path + \".env\")\n",
    "\n",
    "# getenv 함수를 사용해 \"UPSTAGE_API_KEY\"라는 이름의 환경 변수 값을 가져옴.\n",
    "UPSTAGE_API_KEY = getenv(\"UPSTAGE_API_KEY\")\n",
    "\n",
    "# API 키가 성공적으로 로드되었는지 확인하고 메시지를 출력.\n",
    "if UPSTAGE_API_KEY:\n",
    "    print(\"Success API Key Setting!\")\n",
    "else:\n",
    "    print(f\"ERROR: Failed to load UPSTAGE_API_KEY from {base_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8925,
     "status": "ok",
     "timestamp": 1761008981128,
     "user": {
      "displayName": "[서울_3반_배현지]",
      "userId": "06982767720486100463"
     },
     "user_tz": -540
    },
    "id": "bDHrMNuRL_Cl",
    "outputId": "09129daa-72fe-4665-b393-ec929f328c2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVuXv4i4MSpN"
   },
   "source": [
    "# upstage API 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5542,
     "status": "ok",
     "timestamp": 1761008986672,
     "user": {
      "displayName": "[서울_3반_배현지]",
      "userId": "06982767720486100463"
     },
     "user_tz": -540
    },
    "id": "0akQsSDgMmrt",
    "outputId": "5ba13330-b7f2-4300-a67c-60141d0615fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ㅎㅇ! 정말 춥네요~❄️  \n",
      "오늘은 특히 바람이 세게 부는 것 같아요.  \n",
      "따뜻하게 입고 다니시나요? ☕️  \n",
      "(혹시 추워서 단거 당긴다? 핫초코 한 잔의 유혹을 느껴봐요...💕)  \n",
      "\n",
      "> *추위에 맞서는 작은 팁*  \n",
      "> - 목이 따뜻해야 체온 유지에 도움됨! 목도리 필수!  \n",
      "> - 카페인에 약한 사람은 생강차 추천~ 몸이 따뜻해져요!  \n",
      "\n",
      "감기 조심하시고, 오늘 하루 잘 버텨봐요! 💪🔥"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=UPSTAGE_API_KEY,\n",
    "    base_url= \"https://api.upstage.ai/v1\",\n",
    ")\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    # 내가 사용할 모델\n",
    "    model = \"solar-pro2\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\":\"user\",\n",
    "            \"content\": \"ㅎㅇ 춥노\",\n",
    "        }\n",
    "    ],\n",
    "    # 응답을 한 번에 받지 않고 생성되는 대로 조각단위로 실시간 수신\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end='')\n",
    "\n",
    "# stream이 false인 경우\n",
    "# print(stream.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UacPpPqyOJvU"
   },
   "source": [
    "# HTTPX\n",
    "- python 용 HTTP 클라이언트\n",
    "    - request도 httpx\n",
    "- 비동기 통신을 지원\n",
    "    - 여러 API 요청을 보낼 때 하나의 요청이 끝날 때 까지 기다리지 않고 동시에 병렬적으로 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qUcbaSgoME-O"
   },
   "outputs": [],
   "source": [
    "import httpx\n",
    "import asyncio\n",
    "\n",
    "async def call_chat_completion(url, headers, payload):\n",
    "    print(' call_chat_completion 시작')\n",
    "    async with httpx.AsyncClient(timeout=30.0) as client:\n",
    "        # await : 뒤에 작성된 비동기 작업이 종료가 될 때까지 대기\n",
    "        # 만약 다른 비동기 작업이 예정되어 있다면 해당 작업을 수행하기 시작\n",
    "        response = await client.post(url, headers=headers, json=payload)\n",
    "        # HTTP 오류가 발생하면 예외를 발생시키도록\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        return data['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2549,
     "status": "ok",
     "timestamp": 1761008989252,
     "user": {
      "displayName": "[서울_3반_배현지]",
      "userId": "06982767720486100463"
     },
     "user_tz": -540
    },
    "id": "WaKwgUeuQpjK",
    "outputId": "3e8d1a6c-039d-42a0-eee3-e835e4267d74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "태스크 생성\n",
      "아직 upstage API 호출 전\n",
      "태스크 생성\n",
      "아직 upstage API 호출 전\n",
      "이제 각 요청 실행 시작\n",
      " call_chat_completion 시작\n",
      " call_chat_completion 시작\n",
      "모든 요청 완료\n",
      "\n",
      "Response 1\n",
      "물론이죠! 여기 갑니다~  \n",
      "\n",
      "**\"사과가 왜 학교에 갔는지 알아?**  \n",
      "**사과해서!!!\"**  \n",
      "\n",
      "(애플이 사과하는 게 아니라, '사과(謝過)하러' 갔대서... 😅)  \n",
      "\n",
      "혹시 더 필요하신가요? 다른 버전으로 바꿔드릴게요~!\n",
      "====================\n",
      "Response 2\n",
      "프랑스의 수도는 **파리(Paris)**입니다. 파리는 정치, 경제, 문화의 중심지이며, 세계적으로 유명한 관광지(에펠탑, 루브르 박물관 등)와 예술, 역사가 풍부한 도시입니다.  \n",
      "\n",
      "혹시 프랑스나 파리에 대해 더 궁금한 점이 있으면 언제든 물어보세요! 😊\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "# 여러 비동기 작업을 실행하고 결과를 처리하는 메인 함수.\n",
    "async def request(tasks):\n",
    "    print('이제 각 요청 실행 시작')\n",
    "    # asyncio.gather(*tasks): 리스트에 담긴 모든 비동기 작업을 동시에 실행하고,\n",
    "    # 모든 작업이 완료될 때까지 기다린 후 결과를 리스트로 모아서 반환.\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    print('모든 요청 완료')\n",
    "    print()\n",
    "\n",
    "    # 완료된 결과를 하나씩 출력.\n",
    "    for i, res in enumerate(results, 1):\n",
    "        print(f'Response {i}')\n",
    "        print(res)\n",
    "        print('=' * 20)\n",
    "\n",
    "\n",
    "# API 엔드포인트 및 인증 헤더 설정.\n",
    "url = \"https://api.upstage.ai/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {UPSTAGE_API_KEY}\",\n",
    "}\n",
    "\n",
    "# 동시에 보낼 여러 개의 프롬프트를 리스트로 준비.\n",
    "prompts = [\n",
    "    \"농담 하나만 해 줘\",\n",
    "    \"프랑스의 수도는 어디야?\",\n",
    "]\n",
    "\n",
    "# 실행할 비동기 작업들을 담을 리스트.\n",
    "tasks = []\n",
    "# 각 프롬프트에 대해 API 요청 페이로드를 생성.\n",
    "for prompt in prompts:\n",
    "    payload = {\n",
    "        \"model\": \"solar-pro2\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        # stream=False: 전체 응답을 한 번에 받도록 설정.\n",
    "        \"stream\": False,\n",
    "    }\n",
    "    # call_chat_completion 함수를 호출하여 비동기 작업(Task) 객체를 생성하고 리스트에 추가.\n",
    "    # 'await'가 없으므로 함수가 바로 실행되지 않고, 실행 대기 상태의 객체만 만들어짐.\n",
    "    print('태스크 생성')\n",
    "    tasks.append(call_chat_completion(url, headers, payload))\n",
    "    print('아직 upstage API 호출 전')\n",
    "\n",
    "# 준비된 모든 태스크를 동시에 실행.\n",
    "await request(tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0Ac97F0Rokh"
   },
   "source": [
    "### 응답을 JSON으로 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 904,
     "status": "ok",
     "timestamp": 1761008990158,
     "user": {
      "displayName": "[서울_3반_배현지]",
      "userId": "06982767720486100463"
     },
     "user_tz": -540
    },
    "id": "qRxH-4ejRsRB",
    "outputId": "7f7ad447-0b20-4a28-8e9e-915e7ec81a3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured Response:\n",
      "capital: 서울\n",
      "translation: Seoul\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# LLM의 응답을 구조화된 JSON 형식으로 강제하기 위한 설정.\n",
    "response_format = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"수도 정보\",\n",
    "        \"strict\": True,\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"capital\": {\"type\": \"string\"},\n",
    "                \"translation\": {\"type\": \"string\", \"description\": \"수도의 영어 번역\"},\n",
    "            },\n",
    "            \"required\": [\"capital\", \"translation\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# client.chat.completions.create 메서드를 호출하여 LLM에 요청.\n",
    "response = client.chat.completions.create(\n",
    "    model=\"solar-pro2\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"한국의 수도는 어디야?\",\n",
    "        }\n",
    "    ],\n",
    "    # 위에서 정의한 JSON 스키마를 적용하여 응답 형식을 강제.\n",
    "    response_format=response_format,\n",
    ")\n",
    "\n",
    "# LLM의 응답은 JSON 형식의 '문자열'이므로,\n",
    "# json.loads를 사용하여 파이썬 딕셔너리 객체로 변환.\n",
    "structured_dictionary = json.loads(response.choices[0].message.content)\n",
    "\n",
    "# 딕셔너리로 변환된 구조화된 응답을 출력.\n",
    "print(\"Structured Response:\")\n",
    "for key, value in structured_dictionary.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAAHcahZWBM-"
   },
   "source": [
    "# 데이터 합성\n",
    "\n",
    "- LLM을 이용해 인공 데이터를 만드는 작업\n",
    "    - 데이터 증강은 이미 있는 데이터를 변형시켜 양을 늘리는 작업\n",
    "- 학습 데이터가 부족하거나 실제 데이터를 수집하기 어려울 때\n",
    "\n",
    "## 기본 구조\n",
    "\n",
    "효과적인 프롬프트를 작성하는 가이드라인\n",
    "\n",
    "1. 역할(role): AI에게 특정 역할을 부여하고 답변의 전문성과 톤앤매너를 설정\n",
    "\n",
    "ex. 영화광, 영화 평론가 .. 등\n",
    "\n",
    "2. 목표(task): AI가 수행해야 할 작업을 구체적이고 명확하게 지시\n",
    "\n",
    "3. 조건(constraints): 답변 형식, 스타일, 내용 등에 대한 제약 조건을 설정\n",
    "\n",
    "## 데이터 합성 프롬프트의 핵심\n",
    "\n",
    "1. 다양성: 모델의 무작위성을 조절하여 새롭고 다채로운 데이터 생성\n",
    "- `temperature` : 확률분포의 모양을 조절\n",
    "- `top_p` : 핵심 샘플링이라고도 부름. 후보 단어의 범위를 동적으로 조절\n",
    "    - 적당성이 확보되는 수준으로 조절해야함\n",
    "2. 일관성 : 생성된 데이터가 일관된 구조 (JSON 같이)를 가지도록 프롬프트에 명시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIuuXeMUZa3s"
   },
   "source": [
    "### 데이터 생성 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2881,
     "status": "ok",
     "timestamp": 1761011514571,
     "user": {
      "displayName": "[서울_3반_배현지]",
      "userId": "06982767720486100463"
     },
     "user_tz": -540
    },
    "id": "hoSg7MmxZc5j",
    "outputId": "755da97e-52f5-4528-ca55-2793f26050f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"movie_name\": \"컨저링\",\n",
      "  \"year\": 2013,\n",
      "  \"reason\": \"실화를 바탕으로 한 초자연적 공포가 압권인 작품\",\n",
      "  \"description\": \"한 가족의 집에 깃든 악령을 퇴치하기 위한 에드 & 로레인 워렌 부부의 실화를 각색한 공포물. 실제 사건을 바탕으로 한 생생한 긴장감과 독특한 악령의 이미지가 관객을 압도합니다.\",\n",
      "  \"recommended_reason\": \"와, 이건 진짜 친구한테 꼭 보라고 강요할 만한 영화야! 실제 사건을 바탕으로 해서 더 소름 돋고, 악령이 등장하는 장면들은 아직도 꿈에 나올 것 같아. 특히 '앤' 이라는 악령의 비주얼은 공포 영화 역사상 최고 수준이라니까? 소리 한 번 안 지르고 보는 게 불가능할 걸? 어둠 속에서 혼자 보는 건 절대 비추야 진짜.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Upstage API와 통신하기 위해 openai 라이브러리를 임포트합니다.\n",
    "from openai import OpenAI\n",
    "# LLM의 응답(JSON 형식의 문자열)을 파이썬 딕셔너리로 변환하기 위해 json 라이브러리를 임포트합니다.\n",
    "import json\n",
    "\n",
    "# 시스템 프롬프트: AI 모델의 역할과 기본 행동 지침을 정의합니다.\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "당신은 세상의 모든 영화를 꿰뚫고 있는 영화 전문가 '시네마스터'입니다.\n",
    "사용자의 요청에 맞춰 영화를 추천하는 역할을 맡고 있습니다. 영화는 반드시 하나만 추천합니다.\n",
    "\"\"\"\n",
    "\n",
    "# 추가 규칙 프롬프트: AI 모델의 말투나 답변 스타일 등 세부 규칙을 정의합니다.\n",
    "RULE = \"\"\"\n",
    "친구가 소개 해주는 듯 부드럽고 친근한 말투로 답변합니다.\n",
    "특히, recommended_reason 항목에서는 친구가 엄청 호들갑 떨듯이 설명해 주세요.\n",
    "\"\"\"\n",
    "\n",
    "# 응답 형식 프롬프트: LLM의 답변을 구조화된 JSON 형식으로 강제하기 위한 설정입니다.\n",
    "response_format = {\n",
    "    \"type\": \"json_schema\",      # 응답 형식을 JSON 스키마로 지정합니다.\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"영화 추천\",      # 스키마의 이름을 지정합니다.\n",
    "        \"strict\": True,          # 엄격 모드: 스키마에 맞지 않는 응답이 오면 오류를 발생시킵니다.\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",    # 응답의 최상위 타입이 객체(딕셔너리)임을 지정합니다.\n",
    "            \"properties\": {      # 객체에 포함될 속성들을 정의합니다.\n",
    "                \"movie_name\": {\"type\": \"string\"},\n",
    "                \"year\": {\"type\": \"integer\"},\n",
    "                \"reason\": {\"type\": \"string\"},\n",
    "                \"description\": {\"type\": \"string\", \"description\": \"영화에 대한 설명\"},\n",
    "                \"recommended_reason\": {\"type\": \"string\", \"description\": \"이 영화를 추천하는 추가 이유\"}\n",
    "            },\n",
    "            # 필수적으로 포함되어야 할 속성들을 지정합니다.\n",
    "            \"required\": [\"movie_name\", \"year\", \"reason\", \"description\", \"recommended_reason\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Upstage API 클라이언트를 생성합니다.\n",
    "client = OpenAI(\n",
    "    api_key=UPSTAGE_API_KEY,\n",
    "    base_url=\"https://api.upstage.ai/v1\"\n",
    ")\n",
    "\n",
    "# client.chat.completions.create 메서드를 호출하여 LLM에 요청을 보냅니다.\n",
    "response = client.chat.completions.create(\n",
    "    model=\"solar-pro2\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RULE\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"공포 영화를 추천해줘\"\n",
    "        }\n",
    "    ],\n",
    "    response_format=response_format,  # 위에서 정의한 JSON 응답 형식을 적용합니다.\n",
    "    temperature=0.5,                  # 다양성을 위해 temperature를 0.5로 설정합니다.\n",
    "    max_tokens=1000,                  # 응답의 최대 길이를 1000 토큰으로 제한합니다.\n",
    "    top_p=1.0,                        # top_p를 1.0으로 설정하여 모든 단어를 후보로 고려합니다.\n",
    "    n=1,                              # 1개의 응답만 생성합니다.\n",
    "    frequency_penalty=0.0,            # 특정 단어의 반복을 억제하지 않습니다.\n",
    "    presence_penalty=0.0              # 새로운 주제의 등장을 장려하지 않습니다.\n",
    ")\n",
    "\n",
    "# 응답 결과에서 실제 텍스트 내용만 추출합니다.\n",
    "output = response.choices[0].message.content\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fyeJS7h3bvnq"
   },
   "source": [
    "## 생성 데이터 평가 (LLM as a Judge)\n",
    "\n",
    "- 수 많은 합성 데이터를 사람이 직접 검수하는 것은 시간과 비용이 많이 들기 때문에 LLM을 평가자로 활용하는 기법\n",
    "- 단순히 '좋다/나쁘다' 같은 정량적 평가를 넘어 '왜 그렇게 평가했는지'에 대한 이유도 생성하도록 하여 데이터 개선에 대한 피드백도 요구할 수 있음\n",
    "\n",
    "### 생성 평가의 핵심\n",
    "1. 평가 기준 설정 : 평가의 목적이 무엇인지 명확하게 설정 (지시한 Rule을 명확히 이행했는지)\n",
    "2. 일관성 확보 (temperature=0) : 평가자는 창의적인 답변보다 일관되고 객관적인 평가를 내려야 신뢰가 가능  \n",
    "3. 체계적인 평가 프롬프트 설계 : 평가자에게 필요한 모든 정보를 명확하게 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1934,
     "status": "ok",
     "timestamp": 1761011525495,
     "user": {
      "displayName": "[서울_3반_배현지]",
      "userId": "06982767720486100463"
     },
     "user_tz": -540
    },
    "id": "y4HtJ5p0c8lp",
    "outputId": "6d25da1b-c17b-4c44-e3c1-97bcd3dda82e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 5, 'comment': \"모든 평가 기준을 탁월하게 충족했습니다. '컨저링'을 단일 추천하며, 전문가다운 정확한 정보(연도, 실화 기반 설명)와 친근한 말투(\"}\n"
     ]
    }
   ],
   "source": [
    "# '평가자' 역할을 수행할 LLM에게 제공할 시스템 프롬프트입니다.\n",
    "JUDGE_SYSTEM_PROMPT = \"\"\"\n",
    "당신의 역할은 모델 답변 자동 평가자입니다.\n",
    "\n",
    "1. 입력 형식\n",
    "    - 입력 프롬프트: [instruction]\n",
    "    - 모델 답변: [output]\n",
    "    - 평가 기준: [criteria]\n",
    "\n",
    "2. 작업 지시\n",
    "    - [instruction]에 따른 모델 결과물인 [output]을 평가합니다.\n",
    "    - [output]은 [criteria]를 충족하는지 평가합니다.\n",
    "\n",
    "3. 채점 원칙 (각 기준별 1–5점, 정수만)\n",
    "    - 5점 (탁월): 기준을 완전히 충족. 오류·누락 없음. 구체적이고 실행가능.\n",
    "    - 4점 (우수): 대체로 충족. 사소한 흠만 있음(정확성·구체성·형식 등에서 경미한 누락).\n",
    "    - 3점 (보통): 핵심은 맞지만 눈에 띄는 약점 존재(누락, 모호함, 근거 부족 등).\n",
    "    - 2점 (미흡): 중요한 요구를 여러 곳에서 놓침 또는 오류/비논리 다수.\n",
    "    - 1점 (부적합): 전반적으로 요청과 어긋남, 의미있는 도움/근거 없음, 안전·정책 위반 가능성.\n",
    "\n",
    "4. 출력 형식 (엄격 준수)\n",
    "    - \"score\"는 1–5점의 정수로 평가한다.\n",
    "    - \"comment\"는 한국어 1–3문장으로 평가한다. 구체적이고 실행 가능하게 작성한다.\n",
    "    - 출력 형식은 JSON 형식인 response_format을 준수한다.\n",
    "\"\"\"\n",
    "\n",
    "# 평가자 LLM의 응답 형식을 JSON으로 강제하기 위한 설정입니다.\n",
    "judge_response_format = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"영화 추천 평가자\",\n",
    "        \"strict\": True,\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"score\": {\"type\": \"integer\"},\n",
    "                \"comment\": {\"type\": \"string\", \"description\": \"모델의 답변에 대한 평가 주석\"}\n",
    "            },\n",
    "            \"required\": [\"score\", \"comment\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# 평가자 LLM에게 전달할 사용자 프롬프트 템플릿입니다.\n",
    "USER_PROMPT = \"\"\"\n",
    "입력 프롬프트: {instruction}\n",
    "모델 답변: {output}\n",
    "평가 기준: {criteria}\n",
    "\"\"\"\n",
    "\n",
    "# 평가에 사용할 변수들을 정의합니다.\n",
    "instruction = \"공포 영화를 추천해줘\"\n",
    "# output 변수는 이전 데이터 생성 단계에서 얻은 결과물을 그대로 사용합니다.\n",
    "# output = output\n",
    "\n",
    "# 평가 기준은 생성 모델에게 전달했던 모든 지시사항(시스템 프롬프트, 규칙, 응답 형식)을 조합하여 만듭니다.\n",
    "criteria = SYSTEM_PROMPT + RULE + str(response_format)\n",
    "\n",
    "# 평가자 LLM에게 요청을 보냅니다.\n",
    "response = client.chat.completions.create(\n",
    "    model=\"solar-pro2\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": JUDGE_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            # .format()을 사용해 템플릿에 실제 변수 값들을 채워 넣습니다.\n",
    "            \"content\": USER_PROMPT.format(instruction=instruction, output=output, criteria=criteria)\n",
    "        }\n",
    "    ],\n",
    "    temperature=0,  # 평가의 일관성을 위해 temperature를 0으로 설정합니다.\n",
    "    response_format=judge_response_format\n",
    ")\n",
    "\n",
    "# 평가 결과를 JSON 문자열에서 파이썬 딕셔너리로 변환합니다.\n",
    "judge_output = json.loads(response.choices[0].message.content)\n",
    "\n",
    "# 최종 평가 결과를 확인합니다.\n",
    "print(judge_output)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOuYi4tI3vsKu2+W7vMjpSY",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
