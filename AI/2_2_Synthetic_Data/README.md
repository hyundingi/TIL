여진형 교수님

# Post-training (Instruction-tuning, RLHF)

## 1. Pre-training vs Post-training

- Pre-training : 사전학습
    - 방대한 인터넷 텍스트를 통해 언어와 지식을 배우는 단계
    - 학습 목표 - 다음 단어 예측
    
    잘 하면 파운데이션 모델이 됨
    
- Post-training : 사후 학습
    - 사람이 원하는 방식으로 대화하고, 안전하고 유용하게 만드는 단계
    - 유저의 의도를 파악하고 원하는 답변을 모델이 응답하도록 사후 학습을 진행한다.

## 2. Instruction-tuning

- 사전 학습 후 거대 언어 모델은 유저의 의도와 일치하지 않음
- 이를 해결하기 위해 파인 튜닝을 진행함

**파인 튜닝이란?**

- 이미 사전 학습된 기존 모델에 특정 작업이나 도메인에 맞게 추가로 학습시키는 과정

**Instruction-tuning 이란?**

- 언어모델이 사람이 내린 지시문(instruction)을 따르도록 학습하는 단계
- 정답 레이블이 요구되며, 다양한 태스크를 풀 수 있도록 적응하는 것에 중점을 둠
- 다양한 테스크에서 (지시문, 응답) 상을 모아서 언엄모델을 파인튜닝 한다.
- 새로운(모델이 학습하지 않은) 테스크에서 평가를 진행한다

## 3. Reinforcemen Learning from Human Feedback (RLHF)

### Instruction-tuning의 한계

- 명확한 한계점 중 하나는 ‘정답 데이터를 수집하는 것이 비싸다’인데, 명확한 한계점 말고 다른 문제들은 뭐가 있을가?
    - 개방형/창의정 생성과 같은 테스크에는 정답이 존재하지 않는다
    - 언어 모델링은 모든 토큰 레벨의 오류를 동일하게 취급하지만, 어떤 오류는 다른 오류보다 심하게 작용한다.
    - 사람이 만드는 답변 (정답 레이블)이 최적이 아닐 수 있다.
    - Instruction-tuning을 하더라도, 언어모델 폭포 (LM objective)와 인간의 선호를 만족시키는 것 사이에는 여전히 불일치가 존재한다.

**인간의 선호도를 어떻게 모델링 할 것인가?**

- 문제 : 인간의 판단은 일관성이 떨어지고, 기준이 어긋날 수 있다.
- 해결 방법 : 직접 점수를 매기지 않고, 응답을 비교하는 방식을 활용한다.
- 해당 데이터로 리워드 모델을 학습시키게 된다.

**어떻게 최적화를 할 것인가? (강화학습 - 알파고)**

- 강화학습 (Reinforcement Learning, RL) 분야는 오랫동안 이러한/ 관련된 문제들을 연구해왔음.
- RL이 딥러닝과 게임 플레이에 적용되면서 다시 급증
- 언어모델에 RL을 적용하려는 관심은 비교적 최근인 2019년 부터 이루어짐
    - RL을 언어모델에 적용하는 것이 어렵게 여겨짐 (현재도 그렇다)
    - 언어메델에 적용이 될 수 있는 RL알고리즘이 최근에 나옴

## 4. What’s Next?

### 리워드 모델링의 한계점

- 인간의 선호도는 일관성이 부족함
    - 리워드 해킹은 강화학습에서 자주 발생하는 문제이다.
    - 챗봇들은 정답의 여부와 관계없이 생산적이고 도움이 되어 보이는 정답을 생성하게 된다
    - 이러한 결과들은 환각 문제를 발생시키게 된다.
    - 인간의 선호도 학습한 리워드 모델은 더 일관성이 부족함!

## 4. What’s Next? - DPO

### RLHF 에서 ‘RL’을 제거하자

- Direct Preference Optimization (DPO )

## 4. What’s Next? - RLVR

### RLHF 에서 수학 문제와 같이 답이 분명한 문제들은 정답 여부로 리워드를 주자

- Reinforcement Learning with Verifiable Reward (RLVR)
    - 대표적인 성공 사례 : DeepSeek-R1

# Retrieval-augmented Language Models (Information Retrieval, RAG)

## 1. Basic Concepts of Retrieval-augmented LM

쿼리 Query

- 검색 질의 / retrieval input
    - 언어모델의 질의와 같아야 하는 것은 아니다.

인덱스 index

- 문서나 단락과 같은 검색 가능한 항목들을 체계적으로 정리하여 더 쉽게 찾을 수 있도록 하는 것
- 각 정보 검색 메서드는 인덱싱 과정에서 구축된 인덱스를 활용해 쿼리와 관련 있는 정보를 식별한다.
- Datastore에 있는 수 많은 정보 중에서 주어진 쿼리와 가장 관련성이 높은 정보를 찾아내는 과정

Retrieval-augmented Generation (RAG)

- 사용자의 질문에 답하기 위해, datastore에서 관련 정보를 검색(Retrieval) 해와서, 이를 언어모델이 생성(Generation ) 단계에 함께 활용하는 방법

## 2. Infromation Retrieval

- 정보 검색

정보검색 IR의 활용 

- 웹 서치 & 아이템 서치
    - 서치 엔진 ex. 구글 네이버
    - 이커머스 ex. 아마존 쿠팡
- 추천 시스템
    - OTT 서비스
    - 이커머스
- 검색증강생성 (RAG)
    - RAG는 검색된 문서를 활용하여 더 정확하고 최신의 답변을 생성한다.

Retriever 란?

- 사용자의 질의에 맞는 후보 문서를 저장소에서 찾아오는 모듈
- 검색 증강 언어모델에서 첫 단계 역할을 수행

### Sparse Retriever

### Dense Retriever

위 이미지가 cross / 아래가 bi 이다 

초기 검색은 bi 그 후 cross 

## 3. Retrieval-augmented LM

### **Retrieval-augmented LM 이란?**

- 추론 시 외부 데이터 저장소를 불러와 활용하는 언어모델
- RAG : 정보 검색부터 답변 생성까지의 프레임 워크

### Why Retrieval-augmented LM ?

1. 거대언어모델은 모든 지식을 다 자신의 파라미터에 저장하지 못한다
    - 거대 언어 모델은 pre-training(사전학습) 데이터에 자주 나타나는 쉬운 정보를 기억하는 경향성이 있음
    - RAg 는 자주 등장하지 않는 정보에 대해서 큰 효과를 가져다 줌
        - 언어 모델이 잘 알고 있는 정보인 경우 부정적인 영향을 미칠 수도 있음
2. 거대언어모델이 보유한 지식은 금세 시대에 뒤쳐지며, 갱신이 어렵다.
    - 현재의 지식 편집 메서드들은 확장성이 부족함
    - 반면에, 저장소는 쉽게 업데이트가 가능하며, 확장성도 만족함

1. 거대언어모델의 답변은 해석과 검증이 어려움
2. 기업 내부 정보와 같은 보안 정보는 언어모델 학습에 활용되지 않음
    - 사내 챗봇/기업 내부 시스템에 언어모델을 사용하는 경우 내부 데이터를 학습 시 정보 유출의 위험성이 있음

### Noise Robustness


### Negative Rejection

# LLMs with Tool Usage (LLM Agent, Tool Use, MCP)

## 1. Basic Concepts of LLM Agents

### Agent 이란?

- 센서를 통해 환경을 인지하고 액추에이터를 통해 환경에 대해 액션을 통해 영향을 미치는 것으로 간주될 수 있는 모든 것

### LLM Agent 이란?

- 거대언어모델(LLM)을 핵심 구조로 삼아 환경을 이해하고 행동을 수행하는 에이전트
- LLM-first view: 기존 LLM을 활용한 시스템을 에이전트로 만든다
    - search 에이전트, 심리상담 에이전트, 코드 에이전트
- agent-first-view: LLM을 AI 에이전트에 통합하여, 언어를 활용한 추론과 의사소통을 가능하게 한다.
    - 로봇, 임배디드 에이전트

성공적인 에이전트가 갖추어야 할 요건들

- 도구 사용
- 추론과 계획
- 환경 표현
- 환경 이해
- 상호작용 / 의사소통

## 2. Tool Usage in LLMs

### Tool 이란? (LLM 에이전트를 위한)

- 언어 모델 외부에서 실행되는 프로그램에 연결 되는 함수(function) 인터페이스를 의미한다.
    - LLM은 함수 호출과 입력 인자를 생성함으로써 이 도구를 활용할 수 있다.

### 도구 사용 패러다임

- 도구 사용 (Tool Use) : 두 모드 간의 전환
    - 텍스트 생성 모드
    - 도구 실행 모드
- 도구 사용을 유도하는 방법
    - 추론시 프롬프트
    - 학습 (Training - Tool Learning)

### Tool Learning 방식

- 모방 학습 : 인간의 도구 사용 행동 데이터를 기록함으로써, 언어모델이 인간의 행동을 모방하도록 학습
- 가장 간단하고 직관적인 방식

## 3. Model Context Protocol (MCP)

### Why MCP

- 외부 툴을 활용하는 연구가 급증하면서 회사/모델 마다 각기 다른 툴 호출 방식 및 스키마를 개발
- 문제점
    - 호환성 부족 (모델마다 다름)
    - 재사용 어려움(같은 툴도 다른 모델에서는 다시 정의해야함)

### MCP란?

- 언어 모델이 외부 툴과 상호작용하기 위한 표준화된 방식으로 정의한 프로토콜
- 툴 호출, 응답 전달,  컨택스트 공유를 하나의 공통 규격으로 처리


### MCP 아키텍쳐

- MCP Host
    - 하나 또는 여러 개의 MCP 클라이언트를 조정하고 관리하는 AI 애플리케이션
- MCP client
    - MCP 서버와의 연결을 유지하며 MCP 호스트가 사용할수 있도록 MCP 서버로부터 컨택스트를 가져오는 구성 요소
- MCP Server
    - MCP 클라이언트에게 컨텍스트를 제공하는 프로그램

### MCP 계층 (Layer)

- MCP는 두 개의 계층으로 구성된다
    - 데이터 계층
        - 클라이언트 - 서버 통신을 위한 json - rpc 기반 프로토콜을 정의
        - 라이프 사이클 관리, 툴, 리솟, 프롬프트와 같은 핵심 요소들이 포함된다
    - 전송 계층
        - 클라이언트 서버 간 데이터 교환을 가능하게 하는 통신 매커니즘과 채널을 정의
        - 전송 방식에 특화된 연결 수립, 메세지 프레이밍, 인증이 포함된다
- 개념적으로 데이터 계층은 내부 계층 , 전송 계층은 외부 계층 이다.


### MCP의 장점

- 표준화 : 모든 모델/툴이 동일한 호출 규격 사용
- 확장성 : 새로운 툴 쉽게 추가 가능
- 호환성 : 모델/플랫폼에 상관없이 같은 툴 호출 가능
- 재사용성 : 한 번 정의한 툴을 여러 모델에서 활용 가능
- 투명성 : 호출 과정이 명확히 기록 / 검증됨


현재 .. 

- MCP는 현재 학계/업계 에서도 모두 표준으로 확립
- Web 과 AI 개발을 구분하게 되어 양쪽 생태계 모두에서 표준적이고 호환 가능한 개발이 가능해짐 AI

LLM → 우주 정거장 

정거장에는 규율이 있음 

APPTool → 도킹하려는 우주선 

# AI Agent & Langchain

## 1. Environment Representation & Understanding

환경과 에이전트 활용 예시들

- 챗봇
- 로보틱스
- 임배디드 에이전트
- 게임
- 소프트웨어 개발

에이전트가 환경을 이해하기 위해 필요한 것

- 환경에 접근하기 위한 툴
- 환경의 표현(representation)
- 환경을 이해 / 탐색하기 위한 방법론들


**이미지 기반 표현의 문제점**

- 에이전트로서 좋은 성능을 내려면 세부적인 이해가 중요
    - OCR (광학 문자 인식), 복잡한 레이아웃에서의 그라운딩
- 많은 모델들이 이런 태스크에서 실패 하지만 일정 수준의 학습을 통해 개선이 가능하다.


**복잡한 환경은 어떻게 이해할 수 있을까?**

- 모델은 자신이 상호작용하는 환경에 대해 모든 것을 알고있지 않음
- 일부 지식은 LLM 파라미터 안에 포함되어 있음
    - 코딩 지식, 자주 사용하는 웹사이트 탐색 방법 등
- 다른 지식은 실시간으로 환경과의 상호작용을 통해 발견해야 함

## 2. Reasoning & Planning


## 3. Langchain

### Langchain이란?

- LLM 기반 애플리케이션을 빠르게 개발할 수 있는 오픈 소스 프레임워크
- LLM을 다양한 데이터/툴과 연결하여 강력한 애플리케이션 개발 가능
- 연구와 산업 현장에서 빠르게 표준으로 자리잡음

### Langchain 특징

- 다양한 LLM provider (Openai, google .. ) 와 통합하여 모델/회사별 API 차이를 공통 인터페이스로 관리 가능
- Prompt , Memory, Tools 와 같은 컴포넌트들이 모듈화 되어 있어 재사용성과 확장성 확보
- LangGraphㅡ 기반으로 복잡한 워크 플로우를 시각적으로 설계 및 관리 가능

### Langchain 주요 컴포넌트

- prompt templates : 프롬프트를 구조화
- chains : 여러 단계를 연결한 워크 플로우
- Agents : 동적으로 툴 선택 및 실행
- Memory : 대화 히스토리와 상태 유지
- Tools : 외부 api, db, 계산기 등 연결 ..
